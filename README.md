# m-i-r-homework-1-global-key-and-local-key-detection-of-audio-and-symbolic-music-solved
**TO GET THIS SOLUTION VISIT:** [M.I.R Homework 1-Global key and local key detection of audio and symbolic music Solved](https://www.ankitcodinghub.com/product/m-i-r-homework-1-global-key-and-local-key-detection-of-audio-and-symbolic-music-solved/)


---

ğŸ“© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
ğŸ“± **WhatsApp:** +1 419 877 7882  
ğŸ“„ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;98935&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;M.I.R Homework 1-Global key and local key detection of audio and symbolic music Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
Homework 1

Global key and local key detection of audio and symbolic music

Tonality, or so-called music key, is one of the most important attributes in music. In brief, key refers to two aspects of a musical scale: tonic and mode. The tonic is the first note of a diatonic scale. The mode is usually known as â€˜majorâ€™ key, â€˜minorâ€™ key or so. Tonality is identified by the tonic note and the tonic chord. There are some simplified (yet imprecise) ways to identify a musical key. For example, the tonic note is usually recognized as the first or the last note of a music piece. Moreover, if the chord corresponding to the tonic (i.e., the tonic chord) is a major chord, the music piece is then in major key. On the other hand, if the tonic chord is a minor chord, the music piece is then in minor key. However, this over-simplified way in finding key usually fails in most of the real-world musical data. As a high- level concept, musical key is long-term and context dependent. According to the musical context, a music piece may have a global key as its main key, and local keys which may change several times in the music piece. The detection of global and local keys is still not yet a solved problem in MIR.

In this assignment, we will design some global and local key detection algorithms for global and local music key detection for both audio and symbolic data, with full or limited contextual information. You will learn how to extract audio features, how to deal with MIDI data, and the basic music theory and its computational aspects in this assignment.

The concept of a musical key

Letâ€™s start from the notion of the major and minor scales. Denote T as a tone and S a semitone, a major scale is a note sequence represented as T-T-S-T-T-T-S while a minor scale is T-S-T-T-S-T-T. The functions of these seven notes are tonic, supertonic, median, subdominant, dominant, submediant, and leading tone, respectively (see Figure 1). The major and minor scales are the two most commonly seen diatonic scale. If the tonic of a major scale is C, we then call it a C major scale. If the tonic of a minor scale is C, we then call it a C minor scale (see Figures 2 and 3). In Western classical music, there are in general 24 keys (two modes time 12 tonic notes).

A major scale and a minor scale that have the same tonic are called parallel keys. For example, the parallel minor of a C major key is a C minor key. A major scale and a minor scale that have the same

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="layoutArea">
<div class="column">
key signatures are called relative keys. have For example, the relative minor key of the C major key is the A minor key, the relative minor of E major is C# minor, etc.

(PS: Do not confuse the major/minor key with the major/minor chord. A chord is the co-occurrence of (usually 3) notes, like the major triad and the minor triad, while a key represents the structural information in a diatonic scale.)

</div>
</div>
<div class="layoutArea">
<div class="column">
Figure 1: the diatonic scale. (Figure from: )

Figure 2: the C major scale.

Figure 3: the C minor scale.

</div>
</div>
<div class="layoutArea">
<div class="column">
Prerequisite:

</div>
</div>
<div class="layoutArea">
<div class="column">
The following libraries are suggested for this assignment:

ï· librosa, a Python library for music and audio signal processing: ï· pretty-midi, a Python library for MIDI signal processing:

ï· mir_eval, a Python library for MIR evaluation:

The following datasets will be used:

The GTZAN dataset and Alexander Lerchâ€™s annotation of key :

[Dataset] https://drive.google.com/open?id=1Xy1AIWa4FifDF6voKVutvmghGOGEsFdZ [Annotation] https://github.com/alexanderlerch/gtzan_key

Each sample in the GTZAN dataset is a 30-sec clip of music in 10 different genres. We will use the data from the following 9 genres in the dataset for experiment: blues, country, disco, hip-hop, jazz, metal, pop, reggae, and rock.

</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="layoutArea">
<div class="column">
Schubert Winterreise Dataset (SWD):

[Dataset and Annotation] https://zenodo.org/record/4122060#.YituDHpBy5e

This is a multimodal dataset comprising various representations and annotations of Franz Schubertâ€™s song cycle Winterreise. Schubertâ€™s seminal work constitutes an outstanding example of the Romantic song cycleâ€”a central genre within Western classical music. Some of the versions are unavailable online; our TAs will collect them for you as many as they can.

The GiantStep dataset: [Dataset and Annotation] https://drive.google.com/drive/folders/1D-PKkNWkWIQYcUDQokdzAFU0EL-oa3lc?usp=sharing

In summary, the available data can be downloaded from the following link provided by the TA:

https://drive.google.com/drive/folders/1eS_UUX2MrEbEeTVmiDZwIrSW5VBamNrX

Task 1: Global key detection based on template matching

We assume that the tonic pitch is the one which appears the most often in a music recording. Based on this assumption, the tonic pitch of a music recording can be estimated by the following process:

<ol>
<li>Compute the chromagram ğ™ = [ğ³1, ğ³2, â€¦ ğ³i, â‹¯ , ğ³N]. Each ğ³ğ‘– is a 12-dimensional chroma vector
at the ğ‘–th frame, and ğ‘ is the number of frames in each song.
</li>
<li>Take average of all the chroma vectors over all the time frames in each song and obtain the song-
level chroma vector x (this process is usually referred to as mean pooling)

1ğ‘

ğ± = ğ‘ âˆ‘ ğ³ğ‘–

ğ‘–=1
</li>
<li>The maximal value of the song-level chroma vector indicates the tonal pitch. For example, if the maximal value of x is at the index of the C note, our estimation of the tonic is C.</li>
<li>Based on the estimated tonic, the final step is to find the mode (we consider only major and minor modes in this assignment) with template matching. In this step, the mode is determined by the correlation coefficient ğ‘…(ğ±, ğ²) between ğ± and the binary-valued templates ğ². For example, if the tonic is C, then we consider two mode templates, one for the C major mode and the other for theCminor mode: ğ²CMajor =[101011010101] and ğ²cminor =[101101011010] (The first index of ğ² indicates C note, the second index is C# note, â€¦, and the 12th index is B note.). If we find that ğ‘…(ğ±, ğ²C Major) &gt; ğ‘…(ğ±, ğ²c minor), the estimated key is then C Major. The correlation coefficient between the song-level chroma and the template is defined as</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
âˆ‘12 (ğ± âˆ’ ğ±Ì…)(ğ² âˆ’ ğ²Ì…)

</div>
</div>
<div class="layoutArea">
<div class="column">
ğ‘…(ğ±,ğ²) = ğ‘˜=1 ğ‘˜ ğ‘˜

âˆšâˆ‘12 (ğ± âˆ’ ğ±Ì…)2 âˆ‘12 (ğ² ğ‘˜=1 ğ‘˜ ğ‘˜=1 ğ‘˜

</div>
<div class="column">
âˆ’ ğ²Ì…)2

</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="layoutArea">
<div class="column">
It should be noted that the step 3 and step 4 are interchangeable. We may first compute the correlation coefficients of the song-level chroma and the 12 key templates (for binary templates, there are only 12 templates in total rather than 24, since the template of C Major is the same as the template of a minor) and find the one which achieves the highest correlation coefficient. Then, we compare the values at the two possible tonic positions (e.g., C or A for C major or a minor, respectively). The largest value corresponds to the tonic. You may use some existed package such as scipy.stats.pearsonr in the scipy library to find the correlation coefficients or you can implement it directly (it is not complicated). There are 24 possible keys, and a music piece only has one global key. In Alexander Lerchâ€™s annotation of the GTZAN dataset, the key symbols are indexed as follows (upper case means major key and lower case means minor key):

A A# B C C# D D# E F F# G G#

0 1 2 3 4 5 6 7 8 9 10 11

a a# b c c# d d# e f f# g g#

1213 141516 1718 192021 2223 5. For evaluation of key finding algorithm, first, the raw accuracy is defined as:

ACC = number of correct detection number of all music pieces in the dataset

The raw accuracy is however unable to resolve the ambiguity in key perception. For example, the C major key is easily to be detected as G major key (a perfect-fifth error), A minor key (a relative- major/minor error), or C minor key (a parallel-major/minor key), because these erroneous keys are intrinsically â€œcloseâ€ to C major keys. To solve this issue, we also consider the weighted score, which gives relative weights to the results having relation to the ground key:

Relation to correct key Points Same 1.0 Perfect fifth 0.5 Relative major/minor 0.3 Parallel major/minor 0.2 Other 0.0

Therefore, the weighted accuracy is defined as:

ACC = # Same + 0.5(# Fifth) + 0.3(# Relative) + 0.2 (Parallel) # of all music pieces in the dataset

</div>
</div>
</div>
<div class="page" title="Page 5">
<div class="section">
<div class="layoutArea">
<div class="column">
You can directly use the evaluation function mir_eval.key.evaluate in the mir_eval library.

Besides the binary-valued templates, letâ€™s also consider other designs of the key templates:

The Krumhansl-Schmuckler key-finding algorithm. A more advanced set of templates for key detection is the Krumhansl-Schmuckler (K-S) profile. Instead of using binary-valued templates, we assign values to the template according to human perceptual experiments. The template values are shown in the following Table (see the columns labeled by K-S). The experiment is done by playing a set of context tones or chords, then playing a probe tone, and asking a listener to rate how well the probe tone fit with the context. In this case, we consider using the correlation coefficient between the input chroma features and the K-S profile for key detection. Notice that the major and minor templates are here rendered by different values, so the templates of the C Major and a minor will not be the same. Therefore, in this case we donâ€™t need to probe the tonic first, but just need to find the maximal correlation coefficient among the major profile, minor profile, and the 12 circular shifts of them, respectively. A web resource http://rnhart.net/articles/key-finding/ demonstrates this idea.

</div>
</div>
<div class="layoutArea">
<div class="column">
Major key

Name Binary K-S Name

Tonic 1 6.35 Tonic

</div>
<div class="column">
Minor key Binary

<pre>    1
    0
    1
    1
    0
    1
    0
    1
    1
    0
    1
    0
</pre>
</div>
<div class="column">
K-S 6.33 2.68 3.52 5.38 2.60 3.53 2.54 4.75 3.98 2.69 3.34 3.17

</div>
</div>
<div class="section">
<div class="layoutArea">
<div class="column">
0 2.23

</div>
</div>
<div class="layoutArea">
<div class="column">
Supertonic

</div>
</div>
<div class="layoutArea">
<div class="column">
1 3.48

</div>
</div>
<div class="layoutArea">
<div class="column">
1 4.38

</div>
</div>
<div class="layoutArea">
<div class="column">
Supertonic

</div>
</div>
<div class="layoutArea">
<div class="column">
0 2.33

</div>
</div>
<div class="layoutArea">
<div class="column">
Mediant

</div>
</div>
<div class="layoutArea">
<div class="column">
Mediant

</div>
</div>
<div class="layoutArea">
<div class="column">
Subdominant

</div>
</div>
<div class="layoutArea">
<div class="column">
1 4.09

</div>
</div>
<div class="layoutArea">
<div class="column">
Subdominant

</div>
</div>
<div class="layoutArea">
<div class="column">
0 2.52

</div>
</div>
<div class="layoutArea">
<div class="column">
Dominant

</div>
</div>
<div class="layoutArea">
<div class="column">
1 5.19

</div>
</div>
<div class="layoutArea">
<div class="column">
Dominant

</div>
</div>
<div class="layoutArea">
<div class="column">
0 2.39

</div>
</div>
<div class="layoutArea">
<div class="column">
Submediant

</div>
</div>
<div class="layoutArea">
<div class="column">
Submediant

</div>
</div>
<div class="layoutArea">
<div class="column">
1 3.66

</div>
</div>
<div class="layoutArea">
<div class="column">
0 2.29

</div>
</div>
<div class="layoutArea">
<div class="column">
Leading tone

</div>
</div>
</div>
<div class="layoutArea">
<div class="column">
Leading tone

</div>
<div class="column">
1 2.88

</div>
</div>
<div class="layoutArea">
<div class="column">
The harmonic templates. We assume that the strength of the fundamental frequency of

and the strength of the ğ‘˜th harmonic of a note is ğ›¼ğ‘˜, 0 &lt; ğ›¼ &lt; 1. Consider the harmonic order to seven, then the chroma template of a single C note is

ğ’–ğ’„ =(1+ğ›¼+ğ›¼3 +ğ›¼7,0,0,0,ğ›¼4,0,0,ğ›¼2 +ğ›¼5,0,0,ğ›¼6,0)

</div>
</div>
<div class="layoutArea">
<div class="column">
a note is 1,

</div>
</div>
</div>
</div>
<div class="page" title="Page 6">
<div class="layoutArea">
<div class="column">
And the template of the C Major key is then ğ’–ğ‘ª +ğ’–ğ‘« +ğ’–ğ‘¬ +ğ’–ğ‘­ +ğ’–ğ‘® +ğ’–ğ‘¨ +ğ’–ğ‘©. The harmonic templates for the 24 major/minor keys are therefore constructed in this way.

The data-driven templates. All the above templates are determined by our domain knowledge of music. However, construct the template from real-world data (i.e., the machine learning approach) would be expected as the â€œultimateâ€ solution because our application scenario is always on the real- world data. To develop data-driven key finding algorithms, practical issues include the size of the data, the consistency between the training and testing data, data imbalance, data augmentation, and over- fitting, etc. In this assignment, as a bonus question, we consider an external dataset (the GiantStep dataset) as the training set, and we wish to train the templates from this training set. This process is also known as dictionary learning in the literature of machine learning. Three dictionary learning methods are suggested in this assignment: 1) means of the chroma vectors for each class, 2) random sampling the chroma vectors from each class, and 3) ğ‘˜-means clustering method. See the description of the bonus question in detail.

Q1 (40%) Perform global key finding on the 9 genres in the GTZAN dataset using the feature settings of 1) STFT-based chromagram, 2) CQT chromagram and 3) CENS chromagram and the matching scheme of 1) binary-valued template matching, 2) K-S template matching, and 3) harmonic template matching (you may try ğ›¼ = 0.9). Again, since there is no annotation in the classical genre, you donâ€™t need to run that genre. Report the raw accuracy and weighted accuracy per genre and per method. Which genre achieves better performance and why? Which method appear to be more competitive and why? Discuss your results.

Hint: the chroma features can be obtained from the following functions:

â€¢ librosa.feature.chroma_stft librosa.feature.chroma_cqt librosa.feature.chroma_cens

Q2 (30%) Repeat the process in Q1 on the MIDI data and all the available audio versions (i.e., HU33, SC06, FI66, FI80) of the Schubert Winterreise Dataset. Report the average raw accuracy and weighted accuracy for each version. Is there any difference among the versions? Are MIDI data easier for key finding? Discuss your results.

Hint: for symbolic data, you may use pretty_midi.Instrument.get_chroma to get the chroma vector.

Q3 (bonus) Construct the templates for the 24 major/minor keys using the GiantStep dataset. There are many possible ways to construct the templates. There can also be multiple templates for each key.

</div>
</div>
</div>
<div class="page" title="Page 7">
<div class="layoutArea">
<div class="column">
For example, the template of D major can be constructed by taking the average over all chroma vectors annotatedasDmajorinthedataset.Wecanalsotakethe ğ‘˜-meansalgorithmoverthesechromavectors to obtain ğ‘˜ templates for D major. For the keys not in the dataset, you may consider constructing them by circular shifting from the existing keys. Perform global key finding on the GTZAN dataset using the data-driven template. Does this method benefit some genres? Discuss your results.

Task 2: Local key detection

In the previous task, we assume that one music piece has only one key. This is however not the case for Western classical music, where key modulation is heavily used. That means, the key of a music piece may change over time. In the following task, we will design a key detector that can find the local key, and we will evaluate the algorithm on both audio and symbolic datasets.

Similarly, the raw accuracy and weighted accuracy of local key finding can be defined as

ACC = # of correct detection

# of time instances (detections) in all music pieces

ACC = # Same + 0.5(# Fifth) + 0.3(# Relative) + 0.2 (Parallel) # of all time instances (detections) in all music pieces

Note that these accuracies count the number of time instances rather than the number of pieces.

Q4 (20%): Based on Task 1, design a local key detector that outputs the key of the music every 0.1 second. That means, there is a key detection output for every time step, and in this task, we set the time step be 0.1 second. Perform your method one the MIDI data and all the available audio versions of the Schubert Winterreise Dataset. For simplicity, letâ€™s evaluate the results against the annotator 1. Report the raw accuracy and the weighted accuracy.

Hint: to get the local tonality feature, you may consider the mean-pooled chroma of a segment (maybe 30 seconds or so), not of the whole music piece. For example, the feature representing the local key at the 60th second can be obtained by summing up the chorma vectors from the 45th to the 75th second. You may try the optimal segment size empirically.

Q5 (10%): The local key detection problem can be regarded as a segmentation problem. There has been evaluation metrics for the segmentation performance in the chord recognition problem, but such metrics have not been applied in local key detection. Please apply the over-segmentation, under- segmentation and average segmentation measures (please refer to the directional Hamming divergence and see page 33 in Lecture 3 slides) on the local key detection of the Schubert Winterreise Dataset.

</div>
</div>
</div>
<div class="page" title="Page 8">
<div class="layoutArea">
<div class="column">
Hint: these metrics have been implemented somewhere in mir_eval.chord.

Q6 (bonus): if possible, please design an algorithm that (hopefully can) outperforms the template matching algorithms introduced here. You may use more advanced method (e.g., deep learning) and novel data representations that you may want to create.

Please submit your .zip file containing the report (PDF) and your codes, with the file name â€œHW1_[your ID]â€ to the course website.

The deadline of Assignment 1 is April 26, and we will discuss it on May 3.

</div>
</div>
</div>
